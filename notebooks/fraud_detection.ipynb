{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observations:\n",
    "\n",
    "Size of the DataFrame:\n",
    "    The DataFrame contains 284,807 entries (rows).\n",
    "    There are 31 columns in total.\n",
    "\n",
    "Non-null Counts:\n",
    "    All columns have 284,807 non-null values, indicating there are no missing values in the dataset.\n",
    "\n",
    "Data Types:\n",
    "    The majority of the columns (30 out of 31) are of type float64, which suggests that they hold continuous numerical data.\n",
    "    Only one column, Class, is of type int64, typically indicating a categorical variable or a label (often used in classification tasks).\n",
    "\n",
    "Memory Usage:\n",
    "    The DataFrame uses approximately 67.4 MB of memory, which is relatively efficient for the amount of data it holds.\n",
    "\n",
    "Column Breakdown:\n",
    "    The columns are prefixed with V (from V1 to V28), indicating they may represent different features or variables, commonly found in datasets used for machine learning tasks (e.g., anomaly detection).\n",
    "    The Amount column represents a monetary value, while the Class column suggests the data is for a classification problem, possibly distinguishing between normal and fraudulent transactions since this is a financial dataset.\n",
    "\n",
    "Summary:\n",
    "    The dataset is complete, with no missing values and a mix of float and integer data types. The structure suggests it may be used for predictive modeling in a financial context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in this dataset, but always verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkl for duplicates and remove if present\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since fraud detection is a classification problem, we meed to check if the dataset is imbalanced\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "fraud_count = df['Class'].value_counts(normalize=True) * 100\n",
    "print(fraud_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Observations:\n",
    "Very imbalanced dataset (fraud cases â‰ˆ 0.17% of the total)\n",
    "\n",
    "This confirms we will need oversampling (SMOTE) or undersampling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Statistics:\n",
    "Time: The mean value is 94,811.08 with a standard deviation of 47,481.05, indicating a wide range of time values. The minimum is 0 and the maximum is 172,792, suggesting a time span that likely covers many transactions.\n",
    "\n",
    "V1 to V28: These variables have varying means and standard deviations, indicating different distributions. Most have means close to 0, with some negative and positive values, suggesting they might be standardized features.\n",
    "\n",
    "Amount: The mean amount is 88.47, with a relatively high standard deviation of 250.40, indicating significant variability in transaction amounts. The minimum amount is 0 and the maximum is 25,691.16.\n",
    "\n",
    "Class: This column is binary, with a maximum value of 1 and a minimum of 0, indicating it is used for classification (e.g., normal vs. fraudulent).\n",
    "\n",
    "Distribution Insights:\n",
    "The 25th percentile (Q1) and 75th percentile (Q3) for most variables suggest that many features have values that are tightly clustered around the median (50th percentile), with some outliers, particularly in the V columns.\n",
    "The standard deviations for the V columns vary significantly, indicating some features are more volatile than others.\n",
    "\n",
    "Potential Issues:\n",
    "The high variability in the Amount column relative to its mean suggests that outliers could be influencing the results, which may need further investigation.\n",
    "\n",
    "Overall Structure:\n",
    "The DataFrame contains 31 columns, mostly numerical, indicating it is suitable for machine learning tasks, particularly classification and regression analyses.\n",
    "\n",
    "Summary\n",
    "The dataset is complete. It contains various numerical features with different distributions, indicating great suitability for analysis in a financial context, in this case for detecting anomalies or fraud. The presence of outliers and variability in certain key metrics suggests careful preprocessing will be required for effective modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Amount and Time are not transformed like V1-V28, we normalize them\n",
    "scaler = StandardScaler()\n",
    "df['Scaled_Amount'] = scaler.fit_transform(df[['Amount']])\n",
    "df['Scaled_Time'] = scaler.fit_transform(df[['Time']])\n",
    "\n",
    "# Drop original columns\n",
    "df.drop(columns=['Amount', 'Time'], inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, we have Scaled_Amount and Scaled_Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how features are related:\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Structure:\n",
    "The heatmap displays the correlation values between all pairs of features, with colors indicating the strength and direction of the correlations.\n",
    "\n",
    "Strong Positive Correlations:\n",
    "Look for dark red areas, which indicate strong positive correlations (close to 1). Features that are highly correlated may provide redundant information for modeling.\n",
    "\n",
    "Strong Negative Correlations:\n",
    "Dark blue areas represent strong negative correlations (close to -1). These features move in opposite directions, which can be insightful for understanding relationships in the data.\n",
    "\n",
    "Weak Correlations:\n",
    "Lighter colors (pale blue or white) indicate weak correlations (close to 0). These features may not provide significant predictive power and could be candidates for removal.\n",
    "\n",
    "Class Correlation:\n",
    "Investigate how the Class feature correlates with other features. A strong correlation (positive or negative) with Class is particularly important, as it indicates potential predictors for classification.\n",
    "\n",
    "Scaled Features:\n",
    "The correlations for Scaled_Amount and Scaled_Time should be examined. Their relationships with other features can provide insights into how these scaled values interact with the original features.\n",
    "\n",
    "Multicollinearity:\n",
    "If several V features show high correlations with each other, it suggests multicollinearity, which might complicate model training and interpretation.\n",
    "\n",
    "Feature Selection:\n",
    "The heatmap guides feature selection and engineering. Features with low correlation to the target variable (Class) or high redundancy may be dropped to simplify the model.\n",
    "\n",
    "Summary\n",
    "The correlation matrix heatmap provides a visual representation of the relationships among features. Key observations include identifying strong positive or negative correlations, assessing the relevance of features to the target variable, and recognizing multicollinearity. This analysis will inform our decisions regarding feature selection and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "# Fraud transactions often have outliers\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='Class', y='Scaled_Amount', data=df)\n",
    "plt.title(\"Transaction Amount Distribution by Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Observations:\n",
    "\n",
    "Class Distribution:\n",
    "The plot shows two distinct classes: 0 and 1. Class 0 likely represents normal transactions, while Class 1 may represent fraudulent transactions.\n",
    "\n",
    "Transaction Amounts:\n",
    "Class 0: The majority of transaction amounts are concentrated around lower values, with some outliers extending significantly higher. This suggests that most legitimate transactions involve smaller amounts, but there are occasional larger transactions.\n",
    "Class 1: The distribution for Class 1 (fraudulent transactions) is notably different, with a much tighter range of amounts. Most fraudulent transactions appear to cluster around very low amounts, with few outliers also present.\n",
    "\n",
    "Outliers:\n",
    "There are significant outliers in Class 0, indicating that while most transactions are small, there are some high-value transactions that might warrant further investigation.\n",
    "The outliers in Class 1 are less pronounced, reflecting a different pattern of fraudulent transactions, which may indicate that fraudsters are often making lower-value transactions.\n",
    "\n",
    "Potential Fraud Patterns:\n",
    "The data suggests that fraudulent transactions (Class 1) are not typically associated with high amounts, which could indicate a strategy of making low-value transactions to avoid detection.\n",
    "\n",
    "Implications for Model Training:\n",
    "The stark difference in transaction amount distributions between the two classes can inform feature selection and engineering. This feature (transaction amount) may be a significant predictor for distinguishing between the two classes.\n",
    "\n",
    "Class Imbalance:\n",
    "If the count of Class 1 (fraudulent transactions) is significantly lower than Class 0, this could indicate a class imbalance issue, which is important to consider during model training.\n",
    "\n",
    "Summary\n",
    "The boxplot reveals distinct distributions of transaction amounts for each class, with Class 0 showing a wide range of higher amounts and Class 1 clustering around lower values. This difference can inform modeling strategies and highlight potential patterns in fraudulent behavior, as well as suggest the importance of the transaction amount feature in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning and scaling, save the dataset\n",
    "df.to_csv(\"../data/processed_creditcard.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
